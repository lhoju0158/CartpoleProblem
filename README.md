# Reinforment Learning with Stable Baselines 3
This code serves as an experimental comparison of the performance of the PPO, A2C, and DQN algorithms within the CartPole environment.

## Prerequisties
* python 3.9.0
* stable-baselines3 2.3.2
* gymnasium 0.29.0

## Results
* 5 epochs with PPO
<p align="center">
<img src="https://github.com/user-attachments/assets/37d4b050-18d2-40c0-b81c-961a944f3b74">
</p>

* 10 epochs with PPO
<p align="center">
<img src="https://github.com/user-attachments/assets/a2f0bbca-6ec6-4006-903f-3e0803ea6a88">
</p>



* 100 epochs with PPO
<p align="center">
<img src="https://github.com/user-attachments/assets/b9c9f7d2-0552-4b30-90e6-6b9232d12e5d">
</p>

* 500 epochs with PPO
<p align="center">
<img src="https://github.com/user-attachments/assets/da3c78a5-26fc-43d7-aca7-b874e905a030">
</p>

## Resources
* [GrahlmanMatthew/CartPole-Reinforcement-Learning](https://github.com/GrahlmanMatthew/CartPole-Reinforcement-Learning?tab=readme-ov-file#reinforment-learning-with-stable-baselines-3)
* [Stable Baselines Documentation](https://stable-baselines.readthedocs.io/en/master/)
